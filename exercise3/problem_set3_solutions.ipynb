{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Exercises 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "P(Y = +1 \\mid X_1 = 1, X_2 = 2) &= \\frac{p(X_1 = 1, X_2 = 2 \\mid Y = +1 )P(Y = +1)}{p(X_1 = 1, X_2 = 2)} \\\\\n",
    "&= \\frac{p(X_1 = 1 \\mid Y = +1)p(X_2 = 2 \\mid Y = +1))P(Y = +1)}{p(X_1 = 1, X_2 = 2)},\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "$$p(X_1 = 1, X_2 = 2) = p(X_1 = 1, X_2 = 2 \\mid Y = +1 )P(Y = +1) + p(X_1 = 1, X_2 = 2 \\mid Y = -1 )P(Y = -1)$$\n",
    "\n",
    "Since conditional distributions are Gaussian, and they are assumed to have the same mean and variance for given $Y = y$, we can write\n",
    "\n",
    "$$p(X_1 = x \\mid Y = +1) = p(X_2 = x \\mid Y = +1) =  \\mathcal{N}(x \\mid \\hat{\\mu}_{+},\\hat{\\sigma}^2_{+})$$\n",
    "and\n",
    "$$\n",
    "p(X_1 = x \\mid Y = -1) = p(X_2 = x \\mid Y = -1) =  \\mathcal{N}(x \\mid \\hat{\\mu}_{-},\\hat{\\sigma}^2_{-}).\n",
    "$$ \n",
    "\n",
    "Using this and setting $P(Y = -1)= P(Y = +1) = 1/2 $ (which means that the prior probabilities cancel), we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(Y = +1 \\mid X_1 = 1, X_2 = 2) &= \\frac{\\mathcal{N}(1 \\mid \\hat{\\mu}_{+},\\hat{\\sigma}^2_{+})\\mathcal{N}(2 \\mid \\hat{\\mu}_{+},\\hat{\\sigma}^2_{+})}{\\mathcal{N}(1 \\mid \\hat{\\mu}_{+},\\hat{\\sigma}^2_{+})\\mathcal{N}(2 \\mid \\hat{\\mu}_{+},\\hat{\\sigma}^2_{+}) + \\mathcal{N}(1 \\mid \\hat{\\mu}_{-},\\hat{\\sigma}^2_{-})\\mathcal{N}(2 \\mid \\hat{\\mu}_{-},\\hat{\\sigma}^2_{-})} \\\\\n",
    "&= \\frac{(\\frac{1}{\\sigma_+} \\,e^{-1/(2\\sigma_+^2)}) \\,(\\frac{1}{\\sigma_+} \\, e^{-2^2/(2\\sigma_+^2)})}\n",
    " {(\\frac{1}{\\sigma_+} \\, e^{-1 / (2\\sigma_{+}^2)})\\,(\\frac{1}{\\sigma_+} \\, e^{-2^2/(2\\sigma_+^2)}) + \n",
    " (\\frac{1}{\\sigma_-} \\, e^{-1/(2\\sigma_-^2)})\\,(\\frac{1}{\\sigma_-} \\, e^{-2^2/(2\\sigma_-^2)})} \\\\\n",
    " &= \\frac{\\frac{1}{\\sigma_+^2} e^{-5/(2\\sigma_+^2)}}{\\frac{1}{\\sigma_+^2} e^{-5/(2\\sigma_+^2)} + \\frac{1}{\\sigma_-^2} e^{-5/(2\\sigma_-^2)}} \\\\\n",
    " &= \\frac{\\frac{1}{16} e^{-5/32}}{\\frac{1}{16} e^{-5/32}+ e^{-5/2}} \\approx 0.3944.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates the posterior probability $P(Y = +1 \\mid X_1 = x_1, X_2 = x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the inputs 'sd_plus' and 'sd_minus' are standard deviations, not variances\n",
    "posterior <- function(x, mu_plus = 0, mu_minus = 0, sd_plus = 4, sd_minus = 1) {\n",
    "  joint_plus <- dnorm(x[1], mu_plus, sd_plus) * dnorm(x[2], mu_plus, sd_plus)\n",
    "  joint_minus <- dnorm(x[1], mu_minus, sd_minus) * dnorm(x[2], mu_minus, sd_minus)\n",
    "  return(joint_plus / (joint_plus + joint_minus))  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a grid of points and evaluate the posterior probability at each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid and compute densities for the grid points\n",
    "grid_1d <- seq(-7,7, by = .25) \n",
    "grid_2d <- expand.grid(grid_1d, grid_1d)\n",
    "grid_density1 <- apply(grid_2d, 1, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the result using 'persp' function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAABlBMVEUAAAD///+l2Z/dAAAA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\niASAARAJAAMgEgAGQCQADPgf9lE9mrOREqYAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_matrix <- matrix(grid_density1, nrow = length(grid_1d))\n",
    "persp(grid_1d, grid_1d, grid_matrix, theta = 25, phi = 35, xlab = 'x1', ylab = 'x2', zlab = 'P(Y = +1 | x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the posterior probability for the class $Y = +1$ is low near the origin and increases as we go further away from it. This can be seen also by inspecting the parameters of the classifier. Both class conditional distributions have their expected value at the origin. However, the variance of each feature is larger under the class $Y = +1$ compared to class $Y = -1$ so it is more likely that an observation far from the origin comes from the class $Y = +1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. c)\n",
    "\n",
    "Both the naive Bayes classifier with Gaussian densities\n",
    "  and QDA are based on the Bayes formula \n",
    "  \n",
    "  $$\n",
    "  p(y\\mid \\boldsymbol{x}) = {p(\\boldsymbol{x} \\mid y) \\ p(y) \\over p(\\boldsymbol{x})},\n",
    "  $$\n",
    "  \n",
    "  and therefore, assuming equivalent class distributions $p(y)$, we\n",
    "  only need to show that for the given class-conditional \n",
    "  distribution implied by a naive Bayes classifier $p(\\boldsymbol{x} \\mid y)$,\n",
    "  we can construct a QDA classifier with the same class-conditional\n",
    "  distribution.\n",
    "\n",
    "  Since the naive Bayes model implies that $X_1$ and $X_2$\n",
    "  are independent given $Y$, we are looking for a bivariate\n",
    "  Gaussian density for the QDA model that is equivalent to the\n",
    "  product of two Gaussian densities. This tells us that the\n",
    "  covariance matrix must be diagonal.\n",
    "  \n",
    "  Furthermore, given that the class-conditional distribution under the\n",
    "  QDA model must match that of the NB model, and in particular, that\n",
    "  the means and variances of $X_1$ and $X_2$ must match those of the\n",
    "  NB model, we arrive at the following QDA parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\boldsymbol{\\mu}_+ &= (\\mu_{+,1}, \\mu_{+,2}) = (0,0), \\\\\n",
    "\\boldsymbol{\\mu}_- &= (\\mu_{-,1}, \\mu_{-,2}) = (0,0),\n",
    "\\end{split}\n",
    "$$\n",
    "and class-conditional covariance matrices\n",
    "$$\n",
    "\\begin{split}\n",
    "\\boldsymbol{\\Sigma}_+ &= \\begin{bmatrix}\n",
    "\\sigma_{+,1}^2 & 0 \\\\\n",
    "0 & \\sigma_{+,2}^2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "16 & 0 \\\\\n",
    "0 & 16\n",
    "\\end{bmatrix}, \\\\\n",
    "\\boldsymbol{\\Sigma}_- &= \\begin{bmatrix}\n",
    "\\sigma_{-,1}^2 & 0 \\\\\n",
    "0 & \\sigma_{-,2}^2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Fix $y \\in \\{-1,+1\\}$ and consider the conditional density $p(\\boldsymbol{x}\\,|\\,Y = y)$ with the naive Bayes assumption. Because $\\boldsymbol{\\Sigma}_y$ is a diagonal matrix, we can use exercise 2. (a) from exercise set 2 to notice that this is a density function of a bivariate normal distribution with expected value $\\boldsymbol{\\mu}_y$ and covariance matrix $\\boldsymbol{\\Sigma}_y$: \n",
    "\n",
    "\\begin{split}\n",
    "p(\\boldsymbol{x}\\,|\\,y) &= \\mathcal{N}(x_1 \\mid \\mu_{y,1},\\sigma_{y,1}^2  )\\mathcal{N}(x_2 \\mid \\mu_{y,2},\\sigma_{y,2}^2 ) \\quad\\quad \\text{Naive assumption} \\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma_{y,1}^2}}\\exp\\left(-\\frac{(x_1-\\mu_{y,1})^2}{2\\sigma_{y,1}^2}\\right)\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma_{y,2}^2}}\\exp\\left(-\\frac{(x_2-\\mu_{y,2})^2}{2\\sigma_{y,2}^2}\\right) \\\\\n",
    "&= \\frac{1}{2\\pi\\sigma_{y,1}\\sigma_{y,2}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^2 \\frac{(x_i - \\mu_{y,i})^2}{\\sigma_{y,i}^2}\\right)\\\\\n",
    "&= \\frac{1}{2\\pi |\\boldsymbol{\\Sigma}_y|^{1/2}}\n",
    "\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_y)^T \\boldsymbol{\\Sigma}_y^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_y) \\right) \\quad \\text{Exercises 2,  problem 2.a)}\n",
    "\\end{split}\n",
    "\n",
    "This demonstrates that a Gaussian naive Bayes is a special case of QDA with diagonal covariance matrices (this is a naive Bayes assumption that predictors are independent given a class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking at a classification problem with $p$ predictors and $k$ classes.\n",
    "\n",
    "Consider first the number of parameters required by the naive Bayes classifier. Fix the class $y \\in \\{1, \\ldots , k\\}$. Now the number of parameters required to specify the class conditional distribution $P(\\boldsymbol{X} \\mid Y = y) =  P(X_1 \\mid Y =  y)\\cdot \\ldots \\cdot P(X_p \\mid Y = y)$ is $2p$, since each $P(X_i \\mid Y =  y)$ is univariate Gaussian, thus requiring two parameters (the expected value $\\mu_{y,i}$ and variance $\\sigma_{y,i}^2$). We have $k$ classes, so the total number of parameters required by naive Bayes is $2kp$.\n",
    "\n",
    "In QDA, each class conditional distribution, $P(\\boldsymbol{X} \\mid Y = y)$, is a multivariate normal distribution with expected value $\\boldsymbol{\\mu}_y = (\\mu_{y,i}, \\ldots \\mu_{y,p})$ and a covariance matrix $\\boldsymbol{\\Sigma}_y \\in \\mathbb{R}^{p \\times p}$. The covariance matrix is symmetric, so it has $p(p+1)/2$ free parameters ($p$ variances on the diagonal, and $p(p-1)/2$ covariances). This shows that we need in total $k(p + p(p+1)/2) = k(p^2 + 3p)/2 $ parameters in QDA.\n",
    "\n",
    "In addition to these, both QDA and naive Bayes use $k-1$ free parameters for the class distibution $P(y)$.\n",
    "\n",
    "The dependence of number of the free parameters on the number of predictors is quadratic for QDA, but only linear for naive Bayes. Hence, the general QDA is more prone to overfitting and requires a larger sample size to reach its asymptotic error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function to create the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data <- function(n_sim) {\n",
    "    \n",
    "    # generate the class labels\n",
    "    Y <- sample(0:2, size = n_sim, replace = TRUE, prob = c(0.4,0.3,0.3))  \n",
    "  \n",
    "    X_combinations <- as.matrix(expand.grid(0:1,0:2)) # create all the possible combinations of X1 and X2\n",
    "    X <- matrix(0, nrow = n_sim, ncol = 2) # initialize a matrix for features values\n",
    "  \n",
    "    for(i in 1:n_sim) {\n",
    "    \n",
    "        # select the right class conditional distribution for X1 and X2 based on the sampled class Y[i]  \n",
    "        prob <- switch (Y[i]+1,\n",
    "                        c(0.2,0.1,0.4,0.2,0.0,0.1),\n",
    "                        c(0.6,0.1,0.1,0.1,0.1,0.0),\n",
    "                        c(0.1,0.4,0.3,0.0,0.2,0.0))\n",
    "      \n",
    "        # use this distribution to sample values for X1 and X2\n",
    "        X[i, ] <- X_combinations[sample(1:6, size = 1, replace = TRUE, prob = prob),]\n",
    "      }\n",
    "        \n",
    "      return(list(X = X, Y = Y))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate some data and see if the numbers roughly match the ones suggested in the exercise sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(12345)\n",
    "n <- 100\n",
    "data <- generate_data(n)\n",
    "\n",
    "X <- data$X\n",
    "Y <- data$Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many samples came from the class $Y = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "40"
      ],
      "text/latex": [
       "40"
      ],
      "text/markdown": [
       "40"
      ],
      "text/plain": [
       "[1] 40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(Y == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check how many observations there are with $X_1 = 0$ and $X_2 = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "28"
      ],
      "text/latex": [
       "28"
      ],
      "text/markdown": [
       "28"
      ],
      "text/plain": [
       "[1] 28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(X[,1] == 0 & X[,2] == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These match closely the suggested values. But why do we expect to see around 29 observations with $X_1 = 0$ and $X_2 = 0$? This follows from the law of total probability:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(X_1 = 0, \\ X_2 = 0) &= \\sum_{i = 0}^2 P(X_1 = 0, \\ X_2 = 0 \\mid Y = i)P(Y = i) \\\\\n",
    "&= 0.2 \\cdot 0.4 + 0.6 \\cdot 0.3 + 0.1 \\cdot 0.3 \\\\\n",
    "&= 0.29\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function implements the training of a naive Bayes classifier. The returned value is a fitted naive Bayes classifier which is here an instance of class 'nb'. The input parameters are:\n",
    "- class labels 'Y' and the corresponding feature values 'X'\n",
    "- the number of classes 'n_classes'\n",
    "- a vector containing the cardinalities of the features 'n_class_x'\n",
    "- the smoothing parameter 'a' (corresponding to $\\alpha$ in the exercise sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb <- function(Y, X, n_class = 3, n_class_x = c(2,3), a = 1) {\n",
    "    n <- length(Y) # number of samples in the training data\n",
    "    \n",
    "    # initialize a list to store the estimated class conditional probabilities\n",
    "    likelihood <- vector('list', length = n_class)\n",
    "  \n",
    "    # the same for the class probabilities  \n",
    "    prior <- numeric(n_class)\n",
    "    \n",
    "\n",
    "    for(c in 0:(n_class-1)) {\n",
    "    \n",
    "        n_c <- sum(Y == c) # the number of observations with Y = c \n",
    "        prior[c+1] <- (n_c + a) / (n + n_class * a) # the formula in the exercise sheet\n",
    "        \n",
    "        # consider each feature separately\n",
    "        for(i in seq_along(n_class_x)) {\n",
    "            \n",
    "            likelihood[[c+1]][[i]] <- numeric(n_class_x[i]) # vector whose length is the cardinality of X_i            \n",
    "            \n",
    "            # loop over possible values of X_i\n",
    "            for(j in 0:(n_class_x[i]-1)) {\n",
    "                \n",
    "                n_cj <- sum(Y == c & X[ ,i] == j) # count how many times we observe X_i = j when Y = c\n",
    "                likelihood[[c+1]][[i]][j+1] <- (n_cj + a) / (n_c + n_class_x[i] * a) # estimated probability    \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ret <- list(prior = prior, likelihood = likelihood, n_class = n_class, n_class_x = n_class_x)\n",
    "    class(ret) <- 'nb' # define that the list 'ret' belongs to class 'nb' \n",
    "    \n",
    "    return(ret)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply our classifier to data generated in part a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- 1 # laplace smoothing\n",
    "trained_nb <- nb(Y,X, a = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated distribution over classes, with $\\alpha = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.398058252427184</li>\n",
       "\t<li>0.349514563106796</li>\n",
       "\t<li>0.252427184466019</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.398058252427184\n",
       "\\item 0.349514563106796\n",
       "\\item 0.252427184466019\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.398058252427184\n",
       "2. 0.349514563106796\n",
       "3. 0.252427184466019\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.3980583 0.3495146 0.2524272"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_nb$prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the estimated conditional probability distribution $P(X_i \\mid Y = c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.325581395348837</li>\n",
       "\t<li>0.511627906976744</li>\n",
       "\t<li>0.162790697674419</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.325581395348837\n",
       "\\item 0.511627906976744\n",
       "\\item 0.162790697674419\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.325581395348837\n",
       "2. 0.511627906976744\n",
       "3. 0.162790697674419\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.3255814 0.5116279 0.1627907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c <- 0\n",
    "i <- 2\n",
    "\n",
    "trained_nb$likelihood[[c + 1]][[i]]\n",
    "sum(trained_nb$likelihood[[c + 1]][[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a prediction function for our naive Bayes classifier. Inputs are: \n",
    "- 'nbfit', a fitted naive Bayes model \n",
    "- 'X' data matrix containing the observations to be classified\n",
    "- 'probablities' boolean determining the output type. If 'TRUE', posterior probabilities are returned, and if 'FALSE', the function returns the class label with the highest posterior probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.nb <- function(nbfit, X, probabilities = FALSE) {\n",
    "  n <- nrow(X)\n",
    "  prob <- matrix(0, nrow = n, ncol = nbfit$n_class) \n",
    "    \n",
    "  for(i in 1:n) # compute joint probabilities p(x,y) = p(x|y)p(y)\n",
    "    for(c in 1:nbfit$n_class) {\n",
    "      prob[i,c] <- nbfit$prior[c] # prior p(y)      \n",
    "      for(j in seq_along(nbfit$n_class_x))\n",
    "        prob[i,c] <- prob[i,c] * nbfit$likelihood[[c]][[j]][X[i,j] + 1] #likelihood p(x_1| y)p(x_2|y) = p(x|y)\n",
    "    }\n",
    "  prob <- prob / rowSums(prob)      # normalize into conditional probabilities p(y|x) \n",
    "  pred <- max.col(prob) - 1        # predict class y as argmax p(y|x)\n",
    "  if(probabilities) prob else pred             \n",
    "}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test set of size $10 000$ and compute the test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test error: 0.4409"
     ]
    }
   ],
   "source": [
    "set.seed(1231)\n",
    "test_data <- generate_data(10000)\n",
    "test_Y <- test_data$Y\n",
    "\n",
    "test_df <- data.frame(Y=test_Y,X1 = factor(test_data$X[,1]),X2 = factor(test_data$X[,2])) # this is used later in part d)\n",
    "\n",
    "y_hat <- predict(trained_nb,test_data$X)\n",
    "\n",
    "cat(\"\\nTest error:\",sum(y_hat != test_Y)/length(test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider different training set sizes and look how this affects the test error. We consider naive Bayes classifiers with different smoothing parameters and multinomial logistic regression (see part d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(12222)\n",
    "\n",
    "# library containing implementation of multinomial logistic regression\n",
    "library('nnet')\n",
    "\n",
    "train_sizes <- 100*2**(-2:6) # considered training sample sizes\n",
    "\n",
    "results <- data.frame(matrix(0,nrow = length(train_sizes), ncol = 4)) # store results\n",
    "rownames(results) <- train_sizes\n",
    "colnames(results) <- c(\"NB_ML\",\"NB_Lap\",\"NB_KT\",\"LogReg\")\n",
    "\n",
    "alphas <- c(0,1,1/2) # smoothing parameters\n",
    "\n",
    "for(jj in seq_along(train_sizes)) {\n",
    "    \n",
    "    # create the data\n",
    "    train_data <- generate_data(train_sizes[jj])\n",
    "    X <- train_data$X\n",
    "    Y <- train_data$Y\n",
    "    \n",
    "    # train & predict for the naive Bayes classifiers with different smoothing parameters\n",
    "    for(ii in seq_along(alphas)) {\n",
    "        \n",
    "            nb_fit <- nb(Y,X,a = alphas[ii])          \n",
    "            y_hat <- predict(nb_fit,test_data$X)\n",
    "            results[jj,ii] <- sum(test_data$Y != y_hat) \n",
    "            \n",
    "    }\n",
    "    \n",
    "    # for the logistic regression, we need to put the data into data frame with predictors defined as factors\n",
    "    train_df <- data.frame(Y = train_data$Y, X1 = factor(train_data$X[ ,1]), X2 = factor(train_data$X[ ,2]))\n",
    "    \n",
    "    # trace = FALSE, convergence information won't be printed\n",
    "    logistic_fit <- multinom(Y ~ X1 + X2, data = train_df, trace = FALSE) \n",
    "    y_hat <- predict(logistic_fit,newdata = test_df)\n",
    "    results[jj,4] <- sum(test_data$Y != y_hat)     \n",
    "    \n",
    "}\n",
    "\n",
    "results <- results/length(test_data$Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAD1BMVEUAAAAAAP8A/wD/AAD/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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplot(results,\n",
    "        type = 'l',\n",
    "        col = c('blue', 'red', 'green', 'black'), \n",
    "        lwd = c(4,4,4,3), \n",
    "        lty = c(1,2,3,4),\n",
    "        ylab = 'test error', \n",
    "        ylim = c(.3, .7),\n",
    "        xaxt = \"n\")\n",
    "\n",
    "axis(1, at=1:nrow(results), labels=rownames(results))\n",
    "\n",
    "legend(\"topright\", \n",
    "       legend = colnames(results),\n",
    "       inset=.05, \n",
    "       col = c('blue', 'red', 'green', 'black'), \n",
    "       lwd = c(4,4,4,3), \n",
    "       lty = c(1,2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that here smoothing does not seem to have any effect on the predicted class labels. However, the estimated posterior probabilities would differ (especially) on small sample sizes. Naive Bayes classifiers seem to converge towards a test error of $0.4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the logistic regression performs quite similarly to naive Bayes and it seems to converge to the same asymptotic value. \n",
    "\n",
    "To check if the naive assumption holds in the generating distribution, we need to recall the definition of conditional independence. Assume that $X_1$, $X_2$ and $Y$ are discrete random variables. We say that $X_1$ is conditionally indepedent of $X_2$ given $Y$ if, and only if\n",
    "\n",
    "$$\n",
    "P(X_1 = x_1, \\ X_2 = x_2 \\mid Y = y) = P(X_1 = x_1 \\mid Y = y)\\cdot P(X_2 = x_2 \\mid Y = y)\n",
    "$$\n",
    "\n",
    "holds for every $x_1,x_2$ and $y$ (such that $P(Y = y) > 0$). Let's check if this holds here. Consider the distribution $P(X_1,X_2 \\mid Y = 2)$. We can observe, that:\n",
    "\n",
    "$$\n",
    "P(X_1 =  0,X_2 = 0 \\mid Y = 2) = 0.1\n",
    "$$\n",
    "\n",
    "but\n",
    "\n",
    "$$\n",
    "P(X_1 = 0 \\mid Y = 2)\\cdot P(X_2 = 0 \\mid Y = 2) = (0.1 + 0.2 + 0.3)\\cdot(0.1 + 0.4) = 0.3 \n",
    "$$\n",
    "\n",
    "This implies that there is dependence between the features (at least when $Y = 2$), so the naive assumption does not hold in the generating distribution. This hints that we should be able to get a lower asymptotic test error if we used a classifier that takes into account the conditional dependencies between the features. Logistic regression does not either explicitly model these dependencies (the relationship between the log-odds and predictors is additive, that is, the effect of changing the value of predictor $X_j$ on the log-odds is independent of the values of the other predictors, see book Chap. 3.3.2). This explains the similar test error on large sample sizes in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure below, the decision tree is grown on the penalty data set. First the data is split into upper half $R_1$ and lower half $\\{R_2, \\dots ,R_6\\}$, then the lower half is split into a left corner $R_2$ and the rest $\\{R_3, \\dots , R_6\\}.$ The numbers in each cell indicate the amount of saved shots (darker green) and the scored ones (lighter green).\n",
    "\n",
    "![](penalties3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gains of the splits using misclassification error as impurity measure are tabulated below. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\text{Split} & Q(D_1) & Q(D_2) & Q(D) & gain(D_1,D_2)\\\\\n",
    "R_1 & 3/52 & 35/116 & 38/168 & 0 \\\\\n",
    "R_2 & 0/21 & 35/95 & 35/116 & 0\\\\\n",
    "R_3 & 6/27 & 29/68 & 35/95 & 0\\\\\n",
    "R_4 & 10/26 & 19/42 & 29/68 & 0\\\\\n",
    "R_5 & 3/15 & 11/27 & 19/42 & 0.119 \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Because the first four splits do not affect the classification in any way (shots in every partition $R_1, \\dots, R_5$) are classified as goals, they do not lead to any improvement in misclassification error. However, in $R_6$ the majority of the shots were saved, so in it the shots are classified as saved. Hence, the last split improves the misclassification error, and so also the gain.\n",
    "\n",
    "This demonstrates why, when growing the decision tree automatically, the quality of the splits is usually measured using ''soft'' measures such as cross-entropy or Gini index. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
